{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbP4T2u7p8eJPAsRbscb1S"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Temporal-Difference methods: model-free learning methods that learn directly from episodes of experience.\n",
        "It is a combination of Monte Carlo and dynamic programming (DP) ideas.\n",
        "\n",
        "Temporal Difference Learning Algorithm Outline:\n",
        "\n",
        "1. Initialize the value function V with some initial values.\n",
        "2. For each time step in an episode:\n",
        "3. Use the policy derived from the current value function to select an action.\n",
        "4. Observe the reward and the next state.\n",
        "5. Update the value of the current state based on the observed reward and the value of the next state.\n",
        "\n",
        "Notice how it uses observed reward and the value of the next state to compute a \"target\", and then updates the value of the current state towards this target. This is different from Monte Carlo methods, which wait until the end of an episode to compute a target based on the total observed return, and from DP methods, which use the expected next value based on a model of the environment.\n",
        "\n",
        "The Sarsa algorithm is a Temporal Difference method that learns a policy that is epsilon-greedy with respect to the current Q-function estimate.\n",
        "\n",
        "Sarsa Algorithm Outline:\n",
        "\n",
        "1. Initialize the Q-function with some initial values.\n",
        "2. For each time step in an episode:\n",
        "Use an epsilon-greedy policy derived from the current Q-function to select an action.\n",
        "3. Observe the reward and the next state.\n",
        "4. Select the next action using the same epsilon-greedy policy.\n",
        "5. Update the Q-value of the current state-action pair based on the observed reward and the Q-value of the next state-action pair."
      ],
      "metadata": {
        "id": "4HZrLFkxL6KZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "def sarsa(action_values, policy, episodes, alpha = 0.1, gamma = 0.99, epsilon = 0.2):\n",
        "\n",
        "    for episode in range(1, episodes + 1):\n",
        "        state = env.reset()\n",
        "        action = policy(state, epilson)\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            next_action = policy(next_state, epilson)\n",
        "\n",
        "            qsa = action_values[state][action]\n",
        "            next_qsa = action_values[next_state][next_action]\n",
        "            action_values[state][action] = qsa + alpha * (reward + gamma * next_qsa - qsa)\n",
        "            state = next_state\n",
        "            action = next_action\n",
        "\n",
        "sarsa(action_values, policy)"
      ],
      "metadata": {
        "id": "a6rcb0cCLlgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code, n_episodes is the number of episodes to train over, alpha is the step size parameter, gamma is the discount factor, and epsilon is the parameter for the epsilon-greedy policy. The function epsilon_greedy is used to select actions, and the Q-function is updated according to the Sarsa update rule.\n",
        "\n",
        "After the function has been run, the Q-function can be used to determine the best action to take in any state by taking the action with the highest Q-value. For example, np.argmax(Q[state]) will give the best action to"
      ],
      "metadata": {
        "id": "I-x9ZHz9Ma4U"
      }
    }
  ]
}