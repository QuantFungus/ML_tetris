{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "wS0wqcooVTKV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Dependencies"
      ],
      "metadata": {
        "id": "oPG-1WPPWSVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaUFXOOjV-7w",
        "outputId": "15704d88-bc9f-4534-d431-48c38fae6c9d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (0.29.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.22.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.7.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import dependencies"
      ],
      "metadata": {
        "id": "OwuT6WBDWXtP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TMOhXvuPVI36"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preperation"
      ],
      "metadata": {
        "id": "zEYVUYwDWZz6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the Policy Network"
      ],
      "metadata": {
        "id": "dpLmm9e-VdKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Policy(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Policy, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "llT5UWkqVjlN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the environment, instantiate the policy network and define the optimizer"
      ],
      "metadata": {
        "id": "ZZzHql1zVoka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the environment\n",
        "env = gym.make('CartPole-v1',render_mode=\"rgb_array\")\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "# Initialize the policy network\n",
        "policy = Policy(state_dim, action_dim)\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.Adam(policy.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "Mos9fZ3XVzHs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Algorithm"
      ],
      "metadata": {
        "id": "gHTWi9VCWlHC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pick an action based on policy"
      ],
      "metadata": {
        "id": "eBBh34JjWobw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def select_action(state):\n",
        "    state = np.array(state)\n",
        "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
        "    probs = policy(state)\n",
        "    m = Categorical(probs)\n",
        "    action = m.sample()\n",
        "    return action.item(), m.log_prob(action)"
      ],
      "metadata": {
        "id": "ds320_LRWvqS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Policy Gradiant Algorithm, the actual training loop"
      ],
      "metadata": {
        "id": "51JQZKsjWyKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_gradient():\n",
        "    num_episodes = 1000\n",
        "    gamma = 0.99\n",
        "\n",
        "    rewards_per_episode = []  # List to store rewards for each episode\n",
        "\n",
        "    # for 1000 episodes\n",
        "    for episode in range(num_episodes):\n",
        "        observations = env.reset()\n",
        "        state = np.array(observations[0])\n",
        "        episode_reward = 0\n",
        "        log_probs = []\n",
        "        rewards = []\n",
        "\n",
        "        # loop through each time step in one episode\n",
        "        while True:\n",
        "            action, log_prob = select_action(state)\n",
        "            next_state, reward, done, _, _ = env.step(action)\n",
        "\n",
        "            log_probs.append(log_prob)\n",
        "            rewards.append(reward)\n",
        "            episode_reward += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "\n",
        "        # Compute the discounted rewards\n",
        "        discounts = [gamma**i for i in range(len(rewards))]\n",
        "        discounted_rewards = [discount * reward for discount, reward in zip(discounts, rewards)]\n",
        "\n",
        "        # Convert the discounted_rewards into a Tensor\n",
        "        discounted_rewards = torch.Tensor(discounted_rewards)\n",
        "\n",
        "        # Normalize the discounted rewards\n",
        "        discounted_rewards -= torch.mean(discounted_rewards)\n",
        "        discounted_rewards /= torch.std(discounted_rewards)\n",
        "\n",
        "        # Calculate the loss\n",
        "        policy_loss = []\n",
        "        for log_prob, reward in zip(log_probs, discounted_rewards):\n",
        "            policy_loss.append(-log_prob * reward)\n",
        "        policy_loss = torch.cat(policy_loss).sum()\n",
        "\n",
        "        # Update the policy network\n",
        "        optimizer.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print the episode statistics\n",
        "        if episode % 10 == 0:\n",
        "            print('Episode {}: reward = {}'.format(episode, episode_reward))\n",
        "\n",
        "    # Plot the rewards per episode\n",
        "    plt.plot(rewards_per_episode)\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Reward')\n",
        "    plt.title('Reward per Episode')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "TIZnJg5yW4oJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Run Trials"
      ],
      "metadata": {
        "id": "vWDyzAqNanmp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy_gradient()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Djyn389ManPG",
        "outputId": "929f01fa-0423-40b2-f2bd-1ef3881bcc55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0: reward = 11.0\n",
            "Episode 10: reward = 31.0\n",
            "Episode 20: reward = 11.0\n",
            "Episode 30: reward = 72.0\n",
            "Episode 40: reward = 31.0\n",
            "Episode 50: reward = 124.0\n",
            "Episode 60: reward = 50.0\n",
            "Episode 70: reward = 36.0\n",
            "Episode 80: reward = 42.0\n",
            "Episode 90: reward = 203.0\n",
            "Episode 100: reward = 112.0\n",
            "Episode 110: reward = 53.0\n",
            "Episode 120: reward = 106.0\n",
            "Episode 130: reward = 93.0\n",
            "Episode 140: reward = 65.0\n",
            "Episode 150: reward = 41.0\n",
            "Episode 160: reward = 87.0\n",
            "Episode 170: reward = 97.0\n",
            "Episode 180: reward = 44.0\n",
            "Episode 190: reward = 62.0\n",
            "Episode 200: reward = 114.0\n",
            "Episode 210: reward = 70.0\n",
            "Episode 220: reward = 83.0\n",
            "Episode 230: reward = 51.0\n",
            "Episode 240: reward = 49.0\n",
            "Episode 250: reward = 39.0\n",
            "Episode 260: reward = 89.0\n",
            "Episode 270: reward = 70.0\n",
            "Episode 280: reward = 46.0\n",
            "Episode 290: reward = 36.0\n",
            "Episode 300: reward = 46.0\n",
            "Episode 310: reward = 61.0\n",
            "Episode 320: reward = 68.0\n",
            "Episode 330: reward = 110.0\n",
            "Episode 340: reward = 135.0\n",
            "Episode 350: reward = 134.0\n",
            "Episode 360: reward = 175.0\n",
            "Episode 370: reward = 115.0\n",
            "Episode 380: reward = 78.0\n",
            "Episode 390: reward = 160.0\n",
            "Episode 400: reward = 388.0\n",
            "Episode 410: reward = 144.0\n",
            "Episode 420: reward = 98.0\n",
            "Episode 430: reward = 63.0\n",
            "Episode 440: reward = 58.0\n",
            "Episode 450: reward = 47.0\n",
            "Episode 460: reward = 64.0\n",
            "Episode 470: reward = 119.0\n",
            "Episode 480: reward = 114.0\n",
            "Episode 490: reward = 115.0\n",
            "Episode 500: reward = 102.0\n",
            "Episode 510: reward = 114.0\n",
            "Episode 520: reward = 63.0\n",
            "Episode 530: reward = 110.0\n",
            "Episode 540: reward = 121.0\n",
            "Episode 550: reward = 114.0\n",
            "Episode 560: reward = 138.0\n",
            "Episode 570: reward = 125.0\n",
            "Episode 580: reward = 119.0\n",
            "Episode 590: reward = 148.0\n",
            "Episode 600: reward = 125.0\n",
            "Episode 610: reward = 111.0\n",
            "Episode 620: reward = 82.0\n",
            "Episode 630: reward = 104.0\n",
            "Episode 640: reward = 106.0\n",
            "Episode 650: reward = 111.0\n",
            "Episode 660: reward = 98.0\n",
            "Episode 670: reward = 81.0\n",
            "Episode 680: reward = 98.0\n",
            "Episode 690: reward = 131.0\n",
            "Episode 700: reward = 151.0\n",
            "Episode 710: reward = 154.0\n",
            "Episode 720: reward = 151.0\n",
            "Episode 730: reward = 156.0\n",
            "Episode 740: reward = 167.0\n",
            "Episode 750: reward = 164.0\n",
            "Episode 760: reward = 161.0\n",
            "Episode 770: reward = 154.0\n",
            "Episode 780: reward = 191.0\n",
            "Episode 790: reward = 250.0\n",
            "Episode 800: reward = 624.0\n"
          ]
        }
      ]
    }
  ]
}