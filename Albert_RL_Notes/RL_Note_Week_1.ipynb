{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Environment Setup"
      ],
      "metadata": {
        "id": "UUtIz5_tajP-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIOBvFrWaWbW",
        "outputId": "f3fd5614-ef2a-4e4a-da6c-5e8d109b4b5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: conda: command not found\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.22.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n"
          ]
        }
      ],
      "source": [
        "!conda install pyopengl pytorch==1.6 matplotlib pandas numpy tqdm jupyter seaborn scikit-learn\n",
        "!pip install gym"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Elements Common to All Control Tasks:\n",
        "\n",
        "\n",
        "*   States S(t)\n",
        "*   Actions A(t)\n",
        "*   Reward R(t)\n",
        "*   Agent\n",
        "*   Environment\n",
        "\n",
        "\n",
        "# Markov Decision Process:\n",
        "*   What is a MDP?\n",
        "It's a Discrete-time (time progresses in finite intervals) stochastic (agent's actions affect only partially the evolution of the task) control process (Based on decision making to achieve potential goal of tasks).\n",
        "\n",
        "\n",
        "Through Markov Decision Proess, we can analyse different tasks in similar processes, by catagorizing tasks into (S,A,R,P), where\n",
        "*   S: set of possible states of the task\n",
        "*   A: set of actions they can be taken in each of the states\n",
        "*   R: set of rewards for each (S,a) pair\n",
        "*   P: Probabilities of passing from one state to another when taking each possible action\n",
        "\n",
        "Markov Process (no memory):\n",
        "$P[S_{t+1}|S_t = s_t] = P[S_{t+1}|S_t=s_t,S_{t-1} = s_{t-1},.....,S_0 = s_0]$\\\\\n",
        "What does the equation above mean? The next state only on the current state, but not previous ones. (Which is the property of Markov Decision)\n",
        "\n",
        "#Types of MDP:\n",
        "*   Finite  (Like tetris)\n",
        "*   Infinite    (Like auto-driving)\n",
        "\n",
        "or\n",
        "*   Episodic    (Terminates in certain circumstances, e.g. chess in checkmate)\n",
        "*   Continuing\n",
        "\n",
        "\n",
        "#Trajectory and Episode\n",
        "\n",
        "Trajectory: elements that are generated when the agent moves from one state to another  (Denoted as $\\tau=S_0,A_0,R_1,S_1,A_1,R_2,S_2,A_2,...$, Showcases the trace of the agent)\n",
        "\n",
        "#Reward vs Return\n",
        "What's the goal? To maximize the long-term sum of rewards. To maximize the episode's return.\n",
        "*   Reward ($R_t$)\n",
        "*   Return ($G_t = R_{t+1}+R_{t+2}+R_{t+3}+....+R_T$)\n",
        "\n",
        "\n",
        "#Discount factor\n",
        "For example, in the maze task, the agent may tried to achieve the goal but not as soon as possible and tried to figure out the shortest path. \\\\\n",
        "To solve this, we can multiply the future rewards by a discount factor $\\gamma$, then $G_t=R_1+\\gamma R_2 + \\gamma ^2 R_3 + ... + \\gamma ^ {T-t-1}R_T$ where $\\gamma \\in [0,1]$.\n",
        "\n",
        "\n",
        "#Policy\n",
        "Policy: A function that decides what action to take in a particular state ($\\pi : S→A$) Input is the state and output is the action in terms of the state\n",
        "\n",
        "*   $\\pi(a|S)$: Probability of taking action a in state S\n",
        "*   $\\pi(S)$: Action a taken in state S\n",
        "\n",
        "Since we want to maximize the long-term reward sum, we have to find optimal policy $\\pi_*$\n",
        "\n",
        "\n",
        "#State Value\n",
        "$v_\\pi (s)=\\mathbb{E}[G_t|S_t=s]$,where $G_t = R_{t+1}+\\gamma R_{t+2}+....+\\gamma ^{T-t-1}R_T$ \\\\\n",
        "being defind as a expected return that we expected to obtain starting from the state $s$, and interacting with the environment following policy $\\pi$ until the end of the episode.\n",
        "*   $v_\\pi (s)$ denotes the expected return\n",
        "*   $\\mathbb{E}[]$  is taken over all possible trajectories or episodes starting from state s and following policy π.\n",
        "*   $G_t$ is the sum of discounted future rewards obtained from time step t onwards.\n",
        "\n",
        "\n",
        "#Evaluating actions in a particular state (q value------State Action Pair)\n",
        "$q_\\pi (s,a)=\\mathbb{E}[G_t|S_t=s,A_t=a]$, where $G_t = R_{t+1}+\\gamma R_{t+2}+....+\\gamma ^{T-t-1}R_T$ \\\\\n",
        "Being defined as an expected return that we expected by taking an action $a$ under the state $s$\n",
        "\n",
        "\n",
        "#Bellman Equation\n",
        "Expanding the definition of State Value, we have: \\\\\n",
        "$$v_\\pi (s) = \\mathbb{E}[G_t|S_t=s] = \\mathbb{E}[R_{t+1}+\\gamma R_{t+2}+....+\\gamma ^{T-t-1}R_T|S_t=s] = \\mathbb{E}[R_{t+1}+\\gamma G_{t+1}|S_t=s] = Σ_a \\pi(a|s) Σ_{s',r}p(s,r|s,a)[r+\\gamma v'_\\pi (s')]$$ \\\\\n",
        "What does the equation above imply? There's a recursive relationship between the state value $v_\\pi (s)$ and value from other states $v'_\\pi(s')$. It expresses the notion of \"bootstrapping,\" where the value of a state is estimated based on the values of its successor states.\n",
        "\n",
        "\n",
        "#Solve a Markov Decision Process:\n",
        "Involving:\n",
        "*   Maximizing value of every state\n",
        "*   Maximizing value of every q-value\n",
        "\n",
        "How? Find the optimal policy\n",
        "Bellman Optimality Equation:\n",
        "$$V_*(s) = max_a Σ_{s',r}p(s',r|s,a)[r+\\gamma v_*(s')]$$\n",
        "$$q_*(s,a) = \\Sigma_{s',r}p(s',r|s,a)[r+\\gamma max_a' q_*(s',a')]$$\n",
        "where p is the probability of taking such action, r+gamma(...) are corresponding reward for reaching such state + discounted optimal value of state/q value \\\\\n",
        "\n",
        "\n",
        "\n",
        "#Environment Setup (CHECK OUT OpenAI GYM Documentation)\n",
        "* State -------> Observation Space\n",
        "* Actions --------> Action Space, step(action)\n",
        "* Reward & Return --------> Reward\n",
        "* Reset() initialize the environment\n",
        "* Trajectory --------> Storing States for each step\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Dynamic Programming\n",
        "How does that relate to RL? Finding the optimal Policy $\\pi_*(s)$.\n",
        "\n",
        "But what do we need?\n",
        "* Optimal Substracture: Finding $\\pi_*$ --------->1) Finding $\\pi_*(S_1)$, 2) Finding $\\pi_*(S_2)$, 3) Finding $\\pi_*(S_3)$, .......\n",
        "\n",
        "* Overlapping Subproblems: The solutions to the subproblems are mutually dependent$$V_*(s) = max_a Σ_{s',r}p(s',r|s,a)[r+\\gamma v_*(s')]$$\n",
        "$$q_*(s,a) = \\Sigma_{s',r}p(s',r|s,a)[r+\\gamma max_a' q_*(s',a')]$$\n"
      ],
      "metadata": {
        "id": "rbXRZ_HibsIE"
      }
    }
  ]
}