{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Monte Carlo Methods\n",
        "Different from Dynamic programming, the policy would take the action by highest q(s,a) instead of v(s,a), since in Monte Carlo Methods, the model is not explicit and we couldn't acquire the exact value of state-transition. Instead, we could only pick actions based on the ESTIMATION of the value, which in this case is q(s,a) \\\\\n",
        "\n",
        "Family of methods that learn optimal v_*(s) or q_*(s,a) values based on Samples \\\\\n",
        "\n",
        "They approximate the values by interacting with the environment to generate sample returns and averaging them \\\\\n",
        "\n",
        "#Advantages\n",
        "* The estimate of one state does not depend on the rest\n",
        "* The cost of estimating the state value is independent of the total number of states\n",
        "* No need a model of Environment\n",
        "\n",
        "#Stochastic Policies\n",
        "* On-policy Learning: Generate samples using the same policy $$ that we are using to optimize\n",
        "* Off-policy Learning: Generate samples with exploratory policy b different from the one we are going to optimize\n",
        "\n",
        "\n",
        "**On Policy Learning** \\\\\n",
        "$\\epsilon$-greedy policy: With probability $系$ select a random action, and with probability $1-系$ select the action with highest Q(s,a)\n",
        "\n",
        "Action is optimal $$(a|s) = 1-系+系_r (when a = a^*)$$\n",
        "Action not optimal $$(a|s) = 系_r (when a \\neq a^*)$$\n",
        "\n",
        "Probability of choosing sub-optimal based on estimation of q-value action: $$系_r=\\frac{系}{|A|}$$\n",
        "\n",
        "\n",
        "#Algorithm:"
      ],
      "metadata": {
        "id": "rlxmESZq9z-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def on_policy_mc_control(policy,action_values,episodes,gamma=0.99,epsilon=0.2):\n",
        "\n",
        "    sa_returns = {} # state-action pair\n",
        "\n",
        "    for episode in range(1,episodes+1):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        transitions = []\n",
        "\n",
        "        while not done:\n",
        "            action = policy(state,epsilon)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            transitions.append([state,action,reward])\n",
        "            state = next_state\n",
        "\n",
        "        G = 0\n",
        "\n",
        "        for state_t, action_t, reward_t in reversed(transitions):\n",
        "            G = reward_t + gamma * G\n",
        "\n",
        "            if not (state_t, action_t) in sa_returns:\n",
        "                sa_returns[(state_t,action_t)] = []\n",
        "\n",
        "            sa_returns[(state_t,action_t)].append(G)\n",
        "            action_values[state_t][action_t] = np.mean(sa_returns[state_t,action_t])"
      ],
      "metadata": {
        "id": "PakjDs6G94f_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Off Policy Learning\n",
        "Find the optimal actions by taking sub-optimal action to explore the environment\\\\\n",
        "\n",
        "For off-policy learning, we separate the exploration from the optimization process and we use different policies for each one \\\\\n",
        "\n",
        "* Exploratory Policy: $b(a|s)$ perform tasks and collect the experience. The experience would be a trajectory containing S,A,R in each time step\n",
        "\n",
        "* Target Policy: $(a|s)$ Updated by sampling the \"experience\" collected by exploratory policy\n",
        "\n",
        "The exploratory policy has to cover all the action that target policy can take. i.e. $if (a|s) > 0, then b(a|s) > 0$\n",
        "\n",
        "**GOAL:** $\\mathbb{E}_b[G_t|S_t=s,A_t=a]=q_b(s,a)$\n",
        "\n",
        "**Importance Sampling:**$W_t=_{k=t}^{T-1}\\frac{\\pi(A_k|S_k)}{b(A_k|S_K)}$\n",
        "\n",
        "**Approach the goal:** $\\mathbb{E}_b[W_tG_t|S_t=s]=v_\\pi(s)$\n",
        "\n",
        "Update the rules:\n",
        "\n",
        "\n",
        "1.   Store G_t for each t and compute the average\n",
        "2.   Everytime observing new return, updating Q value\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6jq2IMUFCtoL"
      }
    }
  ]
}