{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Dynamic Programming\n",
        "How does that relate to RL? Finding the optimal Policy $\\pi_*(s)$.\n",
        "\n",
        "But what do we need?\n",
        "* **Optimal Substracture**: Finding $\\pi_*$ --------->1) Finding $\\pi_*(S_1)$, 2) Finding $\\pi_*(S_2)$, 3) Finding $\\pi_*(S_3)$, .......\n",
        "\n",
        "* **Overlapping Subproblems**: The solutions to the subproblems are **mutually dependent** $$V_*(s) = max_a Σ_{s',r}p(s',r|s,a)[r+\\gamma v_*(s')]$$\n",
        "$$q_*(s,a) = \\Sigma_{s',r}p(s',r|s,a)[r+\\gamma max_a' q_*(s',a')]$$\n",
        "\n",
        "**So what does DP actually do?** \\\\\n",
        "We setup a value table and keep track of state values and q values for each states. Based on the **Bellman Equation**, we do\n",
        "$$V(s)←max_aΣ_{s',r}p(s',r|s,a)[r+\\gamma v_*(s')]$$\n",
        "We first sweep the state space and update the estimated value of the state, then we iteratively update the estimated state value to make more accurate estimation. \\\\\n",
        "\n",
        "**Limitation** Need perfect model for DP, need access to these transition state probabilities  e.g. Rubik's Cube \\\\\n",
        "\n",
        "\n",
        "#Our First DP algorithm: Value Iteration\n",
        "**Goal:** Find the optimal policy $\\pi_*$: $$\\pi_*(s)=arg{max}_aΣ_{s',r}p(s',r|s,a)[r+\\gamma v_*(s')]$$\n",
        "$$\\pi_*(s)←{max}_aΣ_{s',r}p(s',r|s,a)[r+\\gamma v_*(s')]$$\n",
        "\n",
        "\n",
        "Algorithm:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aNFtMHivwdFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# policy_prob initially random, state_value initially all 0,\n",
        "# theta is the tolerance of delta, and gamma is the discounting factor\n",
        "def value_iteration(policy_probs,state_values,theta=1e-6,gamma=0.99):\n",
        "    delta = float('inf')\n",
        "\n",
        "    # when V(s_n+1)-V(s_n) is less than tolerance, that means the estimated state value converges\n",
        "    while delta > theta:\n",
        "        delta = 0\n",
        "        # for each s in S, check the value\n",
        "        for row in range(5):\n",
        "            for col in range(5):\n",
        "                # record the current state value\n",
        "                old_value = state_values[(row,col)]\n",
        "                action_probs = None\n",
        "                # estimate the maximum state value in next step\n",
        "                max_qsa = float(\"-inf\")\n",
        "\n",
        "                for action in range(4):\n",
        "                    # estimate the reward for next state if a certain action has been taken\n",
        "                    next_state,reward,_,_ = env.simulate_step((row,col),action)\n",
        "                    qsa = reward + gamma * state_values[next_state]\n",
        "\n",
        "                    # since this is value iteration, there's always an optimal action for certain state,\n",
        "                    # so we set the possibility into 1, we only choose the action maximizing the return\n",
        "                    if qsa > max_qsa:\n",
        "                        max_qsa = qsa\n",
        "                        action_probs = np.zeros(4)\n",
        "                        action_probs[action] = 1.\n",
        "\n",
        "                state_values[(row,col)] = max_qsa\n",
        "                policy_probs[(row,col)] = action_probs\n",
        "\n",
        "                delta = max(delta,abs(max_qsa - old_value))"
      ],
      "metadata": {
        "id": "pNuP_bqJ95qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Another DP algorithm: Policy Iteration\n",
        "**Policy Iteration: A process that alternatively improves the Estimated Values and the Policy ** \\\\\n",
        "Equation: $$V(s)← Σ_a π(s|a) Σ_{s',r}p(s',r|s,a)[r+\\gamma v_*(s')]$$\n",
        "\n",
        "**PopUp: What is q-function?**\n",
        "$$q_π(s,a)=Σ_{s',r}P(s',r|s,a)[r+γv_π(s')]$$\n",
        "Given current state s and action a, return the sum of probabilities of arriving next state s' after executing the certain action, times the reward obtained by arriving that state plus the discounted value of that state. \\\\\n",
        "Each action has different q-value, and we need to compare different q-values and modify the policy respectively based on larger q-value\n",
        "\n",
        "**Policy Improvement Theorem** \\\\\n",
        "if $$q_π(s,π'(s)) \\geq v_π(s)$$, then $$v_{π'}(s) \\geq v_π(s)$$\n",
        "\n",
        "\n",
        "**Algorithm:**\n",
        "* Policy Evaluation --- Evaluate the current state value according to policy\n",
        "* Policy Improvement --- Use the evaluated state value to modify policy\n"
      ],
      "metadata": {
        "id": "0AAJB0FYBt_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_evaluation(policy_probs,state_values,theta=1e-6,gamma=0.99):\n",
        "    delta = float('inf')\n",
        "    while delta > theta:\n",
        "        delta = 0\n",
        "\n",
        "        # For each s in S(n)\n",
        "        for row in range(5):\n",
        "            for col in range(5):\n",
        "                old_value = state_values[(row,col)]\n",
        "                new_value = 0.\n",
        "                # The probabilities for taking certain action in current state s\n",
        "                action_probabilities = policy_probs[(row,col)]\n",
        "\n",
        "                # For current state, estimating every rewards in the next step, weight them\n",
        "                # by probabilities of each action, and add them up into the state value V(s)\n",
        "                for action,prob in enumerate(action_probabilities):\n",
        "                    next_state,reward,_,_ = env.simulate_step((row,col),action)\n",
        "                    new_value += prob * (reward + gamma * state_values[next_state])\n",
        "\n",
        "                state_values[(row,col)] = new_value\n",
        "\n",
        "                delta = max(delta,abs(old_value-new_value))"
      ],
      "metadata": {
        "id": "cm1FkvTYSWgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_improvement(policy_probs,state_values,gamma=0.99):\n",
        "\n",
        "    policy_stable = True\n",
        "\n",
        "    for row in range(5):\n",
        "        for col in range(5):\n",
        "            # the current action with highest probability\n",
        "            old_action = policy_probs[(row,col)].argmax()\n",
        "\n",
        "            new_action = None\n",
        "            max_qsa = float('-inf')\n",
        "\n",
        "            for action in range(4):\n",
        "                next_state,reward,_,_ = env.simulate_step((row,col),action)\n",
        "                qsa = reward + gamma * state_values[next_state]\n",
        "\n",
        "                if qsa > max_qsa:\n",
        "                    new_action = action\n",
        "                    max_qsa = qsa\n",
        "\n",
        "            action_probs = np.zeros(4)\n",
        "            action_probs[new_action] = 1.\n",
        "            policy_probs[(row,col)] = action_probs\n",
        "\n",
        "            if new_action != old_action:\n",
        "                policy_stable = False\n",
        "\n",
        "    return policy_stable"
      ],
      "metadata": {
        "id": "t9mLK3JzSbTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_iteration(policy_probs,state_values,theta=1e-6,gamma=0.99):\n",
        "    policy_stable = False\n",
        "\n",
        "    while not policy_stable:\n",
        "\n",
        "        policy_evaluation(policy_probs,state_values,theta,gamma)\n",
        "        plot_values(state_values,frame)\n",
        "\n",
        "        policy_stable = policy_improvement(policy_probs,state_values,gamma)\n",
        "        plot_policy(policy_probs,frame)"
      ],
      "metadata": {
        "id": "exaqiqdaSgLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#But what if the Environment becomes a Black Box? Would DP still applicable?\n",
        "\n",
        "#General Policy Iteration\n"
      ],
      "metadata": {
        "id": "D7DZkiZ_VOc4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Monte Carlo Methods\n",
        "Family of methods that learn optimal v_*(s) or q_*(s,a) values based on Samples \\\\\n",
        "\n",
        "They approximate the values by interacting with the environment to generate sample returns and averaging them \\\\\n",
        "\n",
        "#Advantages\n",
        "* The estimate of one state does not depend on the rest\n",
        "* The cost of estimating the state value is independent of the total number of states\n",
        "* No need a model of Environment\n",
        "\n",
        "#Stochastic Policies\n",
        "* On-policy Learning: Generate samples using the same policy $π$ that we are using to optimize\n",
        "* Off-policy Learning: Generate samples with exploratory policy b different from the one we are going to optimize\n",
        "\n",
        "\n",
        "**On Policy Learning** \\\\\n",
        "$\\epsilon$-greedy policy: With probability $ϵ$ select a random action, and with probability $1-ϵ$ select the action with highest Q(s,a)\n",
        "\n",
        "Action is optimal $$π(a|s) = 1-ϵ+ϵ_r (when a = a^*)$$\n",
        "Action not optimal $$π(a|s) = ϵ_r (when a \\neq a^*)$$\n",
        "\n",
        "Probability of choosing sub-optimal based on estimation of q-value action: $$ϵ_r=\\frac{ϵ}{|A|}$$\n",
        "\n",
        "\n",
        "#Algorithm:\n"
      ],
      "metadata": {
        "id": "L_hhNfdzPccn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def on_policy_mc_control(policy,action_values,episodes,gamma=0.99,epsilon=0.2):\n",
        "\n",
        "    sa_returns = {} # state-action pair\n",
        "\n",
        "    for episode in range(1,episodes+1):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        transitions = []\n",
        "\n",
        "        while not done:\n",
        "            action = policy(state,epsilon)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            transitions.append([state,action,reward])\n",
        "            state = next_state\n",
        "\n",
        "        G = 0\n",
        "\n",
        "        for state_t, action_t, reward_t in reversed(transitions):\n",
        "            G = reward_t + gamma * G\n",
        "\n",
        "            if not (state_t, action_t) in sa_returns:\n",
        "                sa_returns[(state_t,action_t)] = []\n",
        "\n",
        "            sa_returns[(state_t,action_t)].append(G)\n",
        "            action_values[state_t][action_t] = np.mean(sa_returns[state_t,action_t])"
      ],
      "metadata": {
        "id": "6LpdyY8P0NzZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}