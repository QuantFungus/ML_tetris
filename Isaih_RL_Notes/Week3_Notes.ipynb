{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQrHWhIB5_BL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Dynamic Programming\n",
        "\n",
        "\n",
        "\n",
        "### Value iteration\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Update rule:\n",
        "\n",
        "\n",
        "\n",
        "$V(s) \\leftarrow max\\sum_{s',r}^{} p(s',r|s,a)[r+\\gamma V(s')]$\n",
        "\n",
        "\n",
        "\n",
        "Moves towards the value under the optimal policy. We follow the following pseudocode:\n",
        "\n",
        "\n",
        "\n",
        "![Value Iteration pseudocode](https://i.imgur.com/xLAssHj.png)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "We'll have a table with the initial values at all states. For each state, the algorithm finds a new value based off of the optimal policy. It terminates when the changes are less than the theta. Here is an example of the algorithm at work:\n",
        "\n",
        "\n",
        "\n",
        "![Value Iteration example](https://i.imgur.com/kaJqfze.png)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "In this example, a ball starts at the top left and must find its way out of the maze. By default, all of the states except for the terminating state at the bottom right have an initial value of -1. The program moves iteratively from the terminating state to update the values of each state.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The maze example follows the following steps to go through the process of value iteration:\n",
        "\n",
        "\n",
        "\n",
        "- Initialize the environment\n",
        "\n",
        "- Define the policy $pi(\\bullet | s)$\n",
        "\n",
        "\t- Create the policy\n",
        "\n",
        "\t- Test the policy with state $(0,0)$\n",
        "\n",
        "\t- See how the random policy does in the maze\n",
        "\n",
        "\t- Plot the policy\n",
        "\n",
        "- Define value table $V(s)$\n",
        "\n",
        "\t- Create the $V(s)$ table\n",
        "\n",
        "\t- Plot $V(s)$\n",
        "\n",
        "- Implement the value iteration algorithm\n",
        "\n",
        "- Show results\n",
        "\n",
        "\t- Show resulting value table $V(s)$\n",
        "\n",
        "\t- Show resulting policy $pi(\\bullet | s)$\n",
        "\n",
        "\t- Test the resulting agent\n",
        "\n",
        "\n",
        "\n",
        "### Policy iteration\n",
        "\n",
        "\n",
        "\n",
        "Give an initial policy and value function, the process of policy iteration will find the optimal policy and values through a series of evaluations and improvements.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Policy iteration can be summarized by the following pseudocode:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "![Policy Iteration Pseudocode](https://i.imgur.com/lfgs9g0.png)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The maze goes through the following steps to go through the process of policy iteration:\n",
        "\n",
        "- Initialize the environment\n",
        "\n",
        "- Define the policy $pi(\\bullet | s)$\n",
        "\n",
        "\t- Create the policy\n",
        "\n",
        "\t- Test the policy with state $(0,0)$\n",
        "\n",
        "\t- See how the random policy does in the maze\n",
        "\n",
        "\t- Plot the policy\n",
        "\n",
        "- Define value table $V(s)$\n",
        "\n",
        "\t- Create the $V(s)$ table\n",
        "\n",
        "\t- Plot $V(s)$\n",
        "\n",
        "- Implement the value iteration algorithm\n",
        "\n",
        "- Show results\n",
        "\n",
        "\t- Show resulting value table $V(s)$\n",
        "\n",
        "\t- Show resulting policy $pi(\\bullet | s)$\n",
        "\n",
        "\t- Test the resulting agent\n",
        "\n",
        "\n",
        "### Bellman variants\n",
        "\n",
        "\n",
        "Policy evaluation iteratively determines the approximate value of a given policy. The formula to approximate the values is identical to the Bellman equation.\n",
        "\n",
        "\n",
        "$V(s) \\leftarrow \\sum_{a}^{} \\pi(s|a) \\sum_{s',r}^{}p(s',r|s,a)[r+\\gamma V(s')]$\n",
        "\n",
        "The algorithm is also very similar to the policy iteration algorithm.\n",
        "\n",
        "\n",
        "![Policy evaluation pseudocode](https://i.imgur.com/VfavhoA.png)\n",
        "\n",
        "There's also policy improvement. There's the decision to change the current policy or to stick with the current one. Policy improvement can be expressed with the following formula:\n",
        "\n",
        "\n",
        "$q_\\pi(s,a) =  \\sum_{s',r}^{}p(s',r|s,a)[r+\\gamma v_\\pi(s')]$\n",
        "\n",
        "\n",
        "Here is the policy improvement theorem. Note that $\\pi$ and $\\pi'$ only differ in the action they take at the same state.\n",
        "\n",
        "If $q_\\pi(s,\\pi'(s)) \\geq v_\\pi(s)$ then $v_{\\pi'}(s)\\geq v_\\pi(s)$\n",
        "\n",
        "$\\pi'(s) =$ arg max $\\sum_{s',r} p(s', r|s,a)[r+\\gamma v_\\pi (s')]$\n",
        "\n",
        "\n",
        "![enter image description here](https://i.imgur.com/tnR8SjJ.png)\n",
        "\n",
        "## DP Summary:\n",
        "\n",
        "![enter image description here](https://i.imgur.com/BKgPgbQ.png)\n",
        "\n",
        "\n",
        "The algorithm will first use policy evaluation. It will generate values for each state in the table. Once policy evaluation is complete, policy improvement will be executed. A policy will be made to determine the action at the states. Another loop of policy evaluation is ran to update the table of values. It keeps looping between evaluation and improvement until no change has been made.\n",
        "\n",
        "The disadvantages of dynamic programming are:\n",
        "\n",
        " - High computational cost\n",
        " - In each sweep we update all the states\n",
        " - Complexity grows very rapidly with the number of states\n",
        "\n",
        "\n",
        "# Monte Carlo methods\n",
        "\n",
        "The agent will use a policy to tackle the task for an entire episode. They approximate the values by interacting with the environment to generate sample returns and averaging them.\n",
        "\n",
        "Advantages of Monte Carlo method:\n",
        "\n",
        " - The estimate of a state does not depend on the rest. The cost of estimating a state value is independent of the total number of states, so the complexity will be much more efficient than DP\n",
        " - We can focus the estimations on the states that help solve the task instead of all the states to avoid the irrelevant ones.\n",
        " - No need to know the dynamics of the environment. For many tasks it is easier to generate samples rather than the dynamics of the environment.\n",
        "\n",
        "\n",
        "Monte Carlo methods depend on the following equation:\n",
        "\n",
        "\n",
        "$\\pi'(s) =$ arg max $q_\\pi(s,a)$\n",
        "\n",
        "\n",
        "TODO: Pickup from here\n"
      ],
      "metadata": {
        "id": "FXDlDPFk-4d0"
      }
    }
  ]
}