{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04QwQaKwA0KZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Resources:\n",
        "[Udemy Reinforcement Learning beginner to master](https://www.udemy.com/course/beginner-master-rl-1/) - Teaches the basics of reinforcement learning. Provides code to experiment with their examples of using RL to solve a maze. Topics include: dynamic programming, Monte Carlo methods, N-step boostrapping, neural networks, Q-Learning, and Advantage Actor-Critic methods. \n",
        "\n",
        "[Udemy Advanced Reinforcement Learning in Python cutting-edge DQNs](https://www.udemy.com/course/advanced-deep-qnetworks/) - Hones in on the lessons taught in the previous course. Also provides more information on PyTorch.\n",
        "\n",
        "[UC Berkely Deep Reinforcement Learning](https://youtube.com/playlist?list=PL_iWQOsE6TfX7MaC6C3HcdOf1g337dlC9) - Is more mathematical but useful for learning all the concepts.\n",
        "\n",
        "\n",
        "\n",
        "# Basic formulas\n",
        "\n",
        "  \n",
        "\n",
        "From last week, the Markov Decision Process using the following formula to calculate the probability of moving from one state to the next:\n",
        "\n",
        "  \n",
        "\n",
        "$P[S_{t+1}|S_t = s_t] = P[S_{t+1}|S_t=s_t,S_{t-1} = s_{t-1},.....,S_0 = s_0]$\n",
        "\n",
        "  \n",
        "\n",
        "In this case, $P[S_{t+1}|S_t]$ is the probability of moving from the state at $t$ to the next state at $t+1$. The next state visited depends only on the current state and not the previous ones.\n",
        "\n",
        "  \n",
        "\n",
        "We also found the rewards can be calculated by the formula:\n",
        "\n",
        "  \n",
        "\n",
        "$G_t=R_1+\\gamma R_2 + \\gamma ^2 R_3 + ... + \\gamma ^ {T-t-1}R_T$\n",
        "\n",
        "  \n",
        "\n",
        "Gamma is the discount factor, somewhere between 0 and 1.\n",
        "\n",
        " # State action pairs and Bellman equation\n",
        "\n",
        "Another important concept is the state-action pair. The value of a state action pair is a q-value and it can be calculated with the following:\n",
        "\n",
        "  \n",
        "\n",
        "$q_\\pi(s,a)=\\mathbb{E}[G_t|S_t=s,A_t=a]$\n",
        "\n",
        "  \n",
        "\n",
        "$q_\\pi(s,a)=\\mathbb{E}[R_1+\\gamma R_2 + \\gamma ^2 R_3 + ... + \\gamma ^ {T-t-1}R_T|S_t=s,A_t=a]$\n",
        "\n",
        "  \n",
        "\n",
        "$\\pi$ is the policy, a function that decides what action to take in a particular state.\n",
        "\n",
        "  \n",
        "\n",
        "Finally is the Bellman equation. Similar to the q-values of state-action pairs, the Bellman equation finds the value of a state.\n",
        "\n",
        "  \n",
        "\n",
        "$v_\\pi(s)=\\mathbb{E}[G_t|S_t=s]$\n",
        "\n",
        "  \n",
        "\n",
        "$v_\\pi(s)=\\mathbb{E}[R_1+\\gamma R_2 + \\gamma ^2 R_3 + ... + \\gamma ^ {T-t-1}R_T|S_t=s]$\n",
        "\n",
        "  \n",
        "\n",
        "$v_\\pi(s)=\\mathbb{E}[R_1+\\gamma G_{t+1}|S_t=s]$\n",
        "\n",
        "  \n",
        "\n",
        "# Markov Chains and application of MDP\n",
        "\n",
        "  \n",
        "\n",
        "At this point, I was also following alone with series of YouTube lectures from UC Berkeley. This professor also considers a new element, the observation ($o_t$). This element is inferred from the current state of the state. Think of it like using a getter function of a class where the state is the class and the output of the getter is the observation.\n",
        "\n",
        "\n",
        " ![diagram 1](https://i.imgur.com/A55DFY7.png)\n",
        "\n",
        "The Markov Decision Process is $\\mathcal{M} = \\{\\mathcal{S}, \\mathcal{A}, \\mathcal{T}, \\mathcal{r} \\}$ within this context. In last week's notes, we went by SARP. P, the probabilities of choosing an action from a state is replaced by the transition operator.\n",
        "\n",
        "Now consider Markov chains. A Markov chain is defined as $\\mathcal{M} = \\{\\mathcal{S}, \\mathcal{T} \\}$. $\\mathcal{S}$ is the set of states and $\\mathcal{T}$ is the transition operator.\n",
        "\n",
        "## TODO and Plans\n",
        "\n",
        "TODO: Review UC Berkely reinforcement learning course. Further understand the practical use of Markov chains and the MDP. According to the plan, up to the 18th should be the last of learning algorithms and concepts. Soon, get started with being able to apply the knowledge to our Tetris game. \n"
      ],
      "metadata": {
        "id": "lWI1kfKmA3Uu"
      }
    }
  ]
}