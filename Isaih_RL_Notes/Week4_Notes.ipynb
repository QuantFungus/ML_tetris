{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWn5Jkl-6SuS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Monte Carlo methods\n",
        "\n",
        "The agent will use a policy to tackle the task for an entire episode. They approximate the values by interacting with the environment to generate sample returns and averaging them.\n",
        "\n",
        "Advantages of Monte Carlo method:\n",
        "\n",
        " - The estimate of a state does not depend on the rest. The cost of estimating a state value is independent of the total number of states, so the complexity will be much more efficient than DP.\n",
        " - We can focus the estimations on the states that help solve the task instead of all the states to avoid the irrelevant ones.\n",
        " - No need to know the dynamics of the environment. For many tasks it is easier to generate samples rather than the dynamics of the environment.\n",
        "\n",
        "\n",
        "Monte Carlo methods depend on the following equation:\n",
        "\n",
        "\n",
        "$\\pi'(s) =$ arg max $\\sum_{s',r} p(s', r|s,a)[r+\\gamma v_\\pi (s')]$\n",
        "\n",
        "\n",
        "Note that this was used before  in the policy improvement theorem.\n",
        "\n",
        "\n",
        "We're going to use q-values of state-action pairs. We'll keep a table of q-values rather than the values of being at a state. The Monte Carlo method is swapping in between policy iteration and policy improvement just like dynamic programming.This continues until the q-values reach their optimal values.\n",
        "\n",
        "\n",
        "The maze goes through the following steps to solve the problem and get an optimal solution by the following:\n",
        "\n",
        " - Initialize the environment\n",
        " - Define value table Q(s,a)\n",
        "\t - Create the Q(s,a) table\n",
        "\t - Plot Q(s,a)\n",
        " - Define the policy\n",
        "\t - Create the policy $\\pi(s)$\n",
        "\t - Test the policy with state (0, 0)\n",
        "\t - Plot the policy\n",
        " - Implement the algorithm\n",
        " - Show results\n",
        "\t - Show resulting value tables Q(s, a)\n",
        "\t - Show resulting policy $\\pi(â€¢|s)$\n",
        "\t - Test the resulting agent\n",
        "\n",
        "![enter image description here](https://i.imgur.com/lHRirvh.png)\n",
        "\n",
        "\n",
        "Monte Carlo method is not too different from dynamic programming. Both use policy evaluation and improvement. However, keep in mind that Monte Carlo method does not depend on previous states for the current value.\n",
        "\n",
        "\n",
        "## Exploration\n",
        "\n",
        "\n",
        "Monte Carlo does not explore every possible path but only the ones it deems \"optimal\" by the q-values. However, there may be an entire path that's more optimal than the chosen one which wasn't chosen because it was behind a sub-optimal q-value.\n",
        "\n",
        "\n",
        "The algorithm has to view all options occasionally. This means to explore all actions and update their estimates on the q-value table. There are two approaches to this.\n",
        "\n",
        " - **Exploring Starts**\n",
        "\t - Every time the agent faces the environment to collect the experience, it starts in a random initial state and will take an initial random action.\n",
        "\t - All q-values will be updated at some point when the state and the action are chosen to start the episode.\n",
        "\t - Not very realistic since there are a lot of tasks we don't have the option.\n",
        " - **Stochastic Policies**\n",
        "\t - $\\pi(a|s)>0, \\forall a \\in A(s)$\n",
        "\t - The policies will sometimes have a probability of choosing every action greater than 0. This ensures that from time to time, it takes an action that it doesn't consider optimal to improve its understanding of the task.\n",
        "\t - This is easier to implement than Exploring Starts\n",
        "\n",
        "\n",
        "\n",
        "On the topic of Schostatic policies, there are two types:\n",
        "\n",
        " - On-policy learning:\n",
        "\t - Generates samples using the same policy $\\pi$ that we're going to optimize.\n",
        " - Off-policy learning:\n",
        "\t - Generates samples with an exploratory policy $b$ different from the one we're going to optimize\n",
        "\n"
      ],
      "metadata": {
        "id": "PON5-TDJ-e9M"
      }
    }
  ]
}