{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Environment Setup"
      ],
      "metadata": {
        "id": "UUtIz5_tajP-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIOBvFrWaWbW",
        "outputId": "f3fd5614-ef2a-4e4a-da6c-5e8d109b4b5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: conda: command not found\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.22.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n"
          ]
        }
      ],
      "source": [
        "!conda install pyopengl pytorch==1.6 matplotlib pandas numpy tqdm jupyter seaborn scikit-learn\n",
        "!pip install gym"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Elements common to all control tasks\n",
        "\n",
        "* **State (S<sub>t</sub>)** - All the relevant information that describes the situation the environment is currently in.\n",
        "\n",
        "\t* A chess board along with what and where the pieces are.\n",
        "\n",
        "\t* A Pac-man game with the location of Pac-man, the ghosts, and the pills/food.\n",
        "\n",
        "* **Action (A<sub>t</sub>)** - The moves the player can perform at any moment in time.\n",
        "\n",
        "\t* The buttons the player presses to move Pac-man.\n",
        "\n",
        "* **Reward (R<sub>t</sub>)** - Numerical value the agent receives after carrying out an action.\n",
        "\n",
        "\t* Pac-man eating a pill should net a positive reward while dying would be a negative reward.\n",
        "\n",
        "* **Agent** - An entity that will participate in the task by observing its state and carrying out actions at each moment of time.\n",
        "\n",
        "\t* The algorithms that carry out the actions.\n",
        "\n",
        "* **Environment** - All the aspects of a task that the agent doesn't control.\n",
        "\n",
        "\t* In chess, the remaining time and opponent's moves would be part of the environment.\n",
        "\n",
        "  \n",
        "  \n",
        "\n",
        "# Markov Decision Process\n",
        "\n",
        "Describe any control task with 4 parts of the acronym S.A.R.P.\n",
        "\n",
        " 1. **S** - Set of possible states of the task\n",
        " 2. **A** - Set of actions that could be taken in each of the states\n",
        " 3. **R** - Set of rewards for each (s,a) pair\n",
        " 4. **P** - Probabilities of passing from one state to another when taking each possible action\n",
        " \n",
        "Take the example of a game of chess. The possible states are all valid configurations of the boards. The actions are the valid moves in each state. The rewards are the rewards obtained by each action in valid states. The set of transitions consists of the probabilities of reaching each successor state by moving a piece.\n",
        "\n",
        "$P[S_{t+1}|S_t = s_t] = P[S_{t+1}|S_t=s_t,S_{t-1} = s_{t-1},.....,S_0 = s_0]$\n",
        "\n",
        "Next state visited depends only on current state and not previous ones.\n",
        "\n",
        "## More vocabulary\n",
        "\n",
        "There are **finite** and **infinite** MDPs, we will focus on finite ones. \n",
        "\n",
        "There's also **episodic** and **continuing** problems. Episodic has a terminating condition, like checkmating, while continuing doesn't have the terminating condition. \n",
        "\n",
        "**Trajectory** is the elements generated when the agent moves from one state to another. **Episode** is the trajectory from the initial state to the terminating condition.\n",
        "\n",
        "The **return** is the net sum of the rewards. The **discount factor** will incentivize taking the most efficient moves.\n",
        "$G_t=R_1+\\gamma R_2 + \\gamma ^2 R_3 + ... + \\gamma ^ {T-t-1}R_T$\n",
        "\n",
        "TODO: **Policy** still doesn't make total sense, get back to it later.\n"
      ],
      "metadata": {
        "id": "rbXRZ_HibsIE"
      }
    }
  ]
}