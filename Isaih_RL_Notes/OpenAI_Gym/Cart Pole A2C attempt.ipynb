{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ef5c725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in /Users/isaihbernardo/miniconda3/lib/python3.8/site-packages (0.29.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/isaihbernardo/miniconda3/lib/python3.8/site-packages (from gymnasium) (1.24.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/isaihbernardo/miniconda3/lib/python3.8/site-packages (from gymnasium) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/isaihbernardo/miniconda3/lib/python3.8/site-packages (from gymnasium) (4.6.3)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/isaihbernardo/miniconda3/lib/python3.8/site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /Users/isaihbernardo/miniconda3/lib/python3.8/site-packages (from gymnasium) (6.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/isaihbernardo/miniconda3/lib/python3.8/site-packages (from importlib-metadata>=4.8.0->gymnasium) (3.11.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b845b363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc5bc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pylab\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5ff769",
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2CAgent:\n",
    "    def __init__(self, state_size, action_size,nodes):\n",
    "        self.render = False # For rendering the cartpole model\n",
    "        self.load_model = False # Set if you want to load a previous checkpoint\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.value_size = 1\n",
    "        self.nodes = nodes\n",
    "\n",
    "        # Policy Gradient hyperparameters\n",
    "        # NOTE: Read more on Policy Gradient\n",
    "\n",
    "        self.discount_factor = 0.99 # For the entire update statement\n",
    "        self.actor_lr = 0.001 # For the Optimizer of Actor\n",
    "        self.critic_lr = 0.005 # Why is it higher? For stability?\n",
    "\n",
    "        # Call the building blocks\n",
    "        self.actor = self.build_actor()\n",
    "        self.critic = self.build_critic()\n",
    "\n",
    "        # Check if we need to load a model\n",
    "        if self.load_model:\n",
    "            self.actor.load_weights(\"./save_model/cartpole_actor.h5\")\n",
    "            self.critic.load_weights(\"./save_model/cartpole_critic.h5\")\n",
    "    # We then create the Neural Network for the approximation of the actor and critic values\n",
    "    # i.e. policy and value for the model.\n",
    "\n",
    "    # NOTE: Actor module: Input of states and outputs the probability of an action (softmax)\n",
    "    def build_actor(self):\n",
    "        actor = Sequential() # Define our model\n",
    "        actor.add(Dense(self.nodes , input_dim = self.state_size, activation= 'relu', kernel_initializer= 'he_uniform'))\n",
    "        actor.add(Dense(self.action_size, activation='softmax',kernel_initializer='he_uniform'))\n",
    "        actor.summary()\n",
    "        actor.compile(loss = 'categorical_crossentropy', optimizer = Adam(lr = self.actor_lr))\n",
    "        return actor\n",
    "    # NOTE: Critic module: Input is also state but the output is also state(linear)\n",
    "    def build_critic(self):\n",
    "        critic = Sequential()\n",
    "        critic.add(Dense(self.nodes, input_dim = self.state_size, activation= 'relu', kernel_initializer= 'he_uniform'))\n",
    "        critic.add(Dense(self.value_size,activation= 'linear', kernel_initializer= 'he_uniform'))\n",
    "        critic.summary()\n",
    "        critic.compile(loss = 'mse', optimizer= Adam(lr=self.critic_lr))# Loss is MSE since we want to give out a value and not a probability.\n",
    "        return critic\n",
    "    # NOTE: We do the function on how the agent will pick the next action and policy based on stochastics(probability)\n",
    "    def get_action(self,state):\n",
    "        policy = self.actor.predict(state,batch_size=1).flatten()\n",
    "        return np.random.choice(self.action_size,1,p=policy)[0]\n",
    "    # NOTE: We do the update for the network policy.\n",
    "    def train_model(self, state, action, reward, next_state, done):\n",
    "        target = np.zeros((1,self.value_size)) # Initialize the policy targets matrix\n",
    "        advantages = np.zeros((1,self.action_size)) # Initialize the advantages matrix\n",
    "\n",
    "        value = self.critic.predict(state)[0] # Get value for this state\n",
    "        next_value = self.critic.predict(next_state)[0] # Get value for the next state\n",
    "\n",
    "        # update the advantages and value tables if done\n",
    "        if done:\n",
    "            advantages[0][action] = reward - value # Basically, what do we gain by choosing the action, will it improve or worsen the advantage\n",
    "            target[0][0] = reward # Fill in the target value to see if we can still improve it in the policy making\n",
    "        else:\n",
    "            advantages[0][action] = reward + self.discount_factor*(next_value) - value # If not yet done, then simply update for the current step.\n",
    "            target[0][0] = reward + self.discount_factor*next_value\n",
    "        # Once we are done with the episode, we then update the weights\n",
    "        self.actor.fit(state,advantages,epochs=1,verbose=0)\n",
    "        self.critic.fit(state,target,epochs=1,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c210c498",
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C_train:\n",
    "    def __init__(self, episodes,nodes):\n",
    "        self.Episodes = episodes\n",
    "        self.nodes = nodes\n",
    "        if __name__ == '__main__':\n",
    "            # TODO: Create an environment\n",
    "            env = gymnasium.make('CartPole-v1', render_mode=\"human\")\n",
    "            # TODO: Get the action and state sizes\n",
    "            state_size = env.observation_space.shape[0]\n",
    "            action_size = env.action_space.n\n",
    "            # TODO: Make the agent by calling the function earlier\n",
    "            agent = A2CAgent(state_size,action_size,self.nodes)\n",
    "            # TODO: Initialize our scores and episodes list\n",
    "            scores, episodes = [], []\n",
    "\n",
    "            # TODO: Create the training loop\n",
    "            for e in range(self.Episodes):\n",
    "                done = False\n",
    "                score = 0\n",
    "                state = env.reset()\n",
    "                state = np.reshape(state,[1,state_size])\n",
    "\n",
    "                while not done:\n",
    "                    # Check if we want to render\n",
    "                    if agent.render:\n",
    "                        env.render()\n",
    "                    action = agent.get_action(state)\n",
    "                    next_state, reward, done, info = env.step(action)\n",
    "                    next_state = np.reshape(next_state,[1,state_size])\n",
    "                    # Give immediate penalty for an action that terminates the episode immediately, Since we want to maximize the time\n",
    "                    # Note that the max for the cartpole is 499 and it will reset, otherwise we keep the current score if it is not yet done, and if it ended we give a -100 reward\n",
    "                    reward = reward if not done or score == 499 else -100\n",
    "                    # We now train the model based on the results of our action taken\n",
    "                    agent.train_model(state,action,reward,next_state,done)\n",
    "                    score += reward\n",
    "                    state = next_state\n",
    "\n",
    "                    if done:\n",
    "                        score = score if score == 500.0 else score +100\n",
    "                        scores.append(score)\n",
    "                        episodes.append(e)\n",
    "                        pylab.plot(episodes,scores,'b')\n",
    "                        pylab.savefig(\"./save_graph/A2C-CartPole.png\")\n",
    "                        if np.mean(scores[-min(10, len(scores)):]) > 490:\n",
    "                            sys.exit()\n",
    "                if e % 50 ==0:\n",
    "                    agent.actor.save_weights(\"./save_model/cartpole_actor.h5\")\n",
    "                    agent.critic.save_weights(\"./save_model/cartpole_critic.h5\")   \n",
    "                    print(\"episode: {} score: {}\".format(e,score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa61081",
   "metadata": {},
   "outputs": [],
   "source": [
    "A2C_train(10, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340c47b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3400c969",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a143ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
