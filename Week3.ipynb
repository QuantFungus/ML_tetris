{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "What is Monte Carlos:\n",
        "sampling multiple episodes or trajectories of the agent's interactions with the environment. An episode refers to a sequence of states, actions, and rewards from the initial state until the termination of the task. By sampling these episodes, Monte Carlo methods can estimate the expected return or cumulative reward for a particular state-action pair or state alone.\n",
        "\n",
        "To estimate the value of a state, Monte Carlo methods average the observed returns obtained from all episodes in which the state was encountered. Similarly, to estimate the value of an action, the observed returns for episodes in which the action was taken from a particular state are averaged. These estimates provide valuable insights into the quality or desirability of states or actions, which enable the agent to make informed decisions.\n",
        "\n",
        "By iteratively updating the value estimates using Monte Carlo methods, RL algorithms can converge towards better policies. The estimated values guide the agent in selecting actions that lead to higher expected returns over time. This iterative process of estimating values and improving policies is a fundamental aspect of RL.\n",
        "\n",
        "Monte Carlo methods in RL offer several advantages. They are model-free, meaning that they do not require a complete understanding of the underlying dynamics of the environment. Instead, they learn directly from interactions. Additionally, Monte Carlo methods are suitable for episodic tasks, where episodes have a natural termination point."
      ],
      "metadata": {
        "id": "tosUaGDCSNJR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PikxIAxhQHpL"
      },
      "outputs": [],
      "source": [
        "def on_policy_mc_control(policy,action_values,episodes,gamma=0.99,epsilon=0.2):\n",
        "    sa_returns = {}\n",
        "    for episode in range(1,episodes+1):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        transitions = []\n",
        "        while not done:\n",
        "            action = policy(state,epsilon)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            transitions.append([state,action,reward])\n",
        "            state = next_state\n",
        "        G = 0\n",
        "        for state_t, action_t, reward_t in reversed(transitions):\n",
        "            G = reward_t + gamma * G\n",
        "            if not (state_t, action_t) in sa_returns:\n",
        "                sa_returns[(state_t,action_t)] = []\n",
        "            sa_returns[(state_t,action_t)].append(G)\n",
        "            action_values[state_t][action_t] = np.mean(sa_returns[state_t,action_t])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Advantages of Monte Carlos:\n",
        "\n",
        "The estimate of a state does not depend on the rest. The cost of estimating a state value is independent of the total number of states (avoid irrelevant states), so more efficient than DP. No need to know the dynamics of the environment."
      ],
      "metadata": {
        "id": "ZNXkW76gQOO2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How does it operate:\n",
        "Monte Carlo explores paths that it perceives as \"optimal\" based on the q-values. However, there might exist an entire path that is more advantageous than the chosen one, but it is disregarded due to a suboptimal q-value.\n",
        "\n",
        "Exploring Starts:\n",
        "Whenever the agent interacts with the environment to gather experience, it begins in a randomly selected initial state and takes an initial random action.\n",
        "All q-values are updated when the state and action are chosen to initiate the episode.\n",
        "This approach may not be very realistic since there are numerous tasks where we lack the option.\n",
        "\n",
        "Stochastic Policies:\n",
        "The policies occasionally assign a non-zero probability to each action. This ensures that periodically the agent takes an action that it doesn't perceive as optimal, thereby enhancing its understanding of the task.\n",
        "Implementing stochastic policies is simpler compared to Exploring Starts.\n",
        "\n",
        "Regarding stochastic policies, there are two types:\n",
        "\n",
        "On-policy learning:\n",
        "Generates samples using the same policy that we aim to optimize.\n",
        "\n",
        "Off-policy learning:\n",
        "Generates samples using an exploratory policy that differs from the one we intend to optimize."
      ],
      "metadata": {
        "id": "kfARMn3yRDt9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interesting Stuff: Monte Carlo method has aspects similar to dynamic programming as both use policy evaluation and improvement."
      ],
      "metadata": {
        "id": "oP3NHywyRf4Y"
      }
    }
  ]
}