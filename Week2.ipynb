{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Value Iteration:\n",
        "\n",
        "Helps find optimal policy: takes action at each state that maximizes return.\n",
        "In order to achieve this, we need to find the optimal values of the states that may follow the present state. At the beginning of the algorithm, we estimate the values of all states. Then, we iterate through all the states that are not the goal state, updating their values according to the formula: $$V(s) = \\max_{a \\in A} \\left\\{ \\sum_{s'r \\in S} p(s',r|s,a) \\left[ r + \\gamma V(s') \\right] \\right\\}$$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ltL2Nhf5CwXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state_values = np.zeros(shape=(5,5))"
      ],
      "metadata": {
        "id": "RO6-rLwjC7Ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def value_iteration(policy_probs, state_values, theta=1e-6, gamma=0.99):\n",
        "    delta = float(\"inf\")\n",
        "    while delta > theta:\n",
        "        delta = 0\n",
        "        for row in range(5):\n",
        "            for col in range(5):\n",
        "                old_value = state_values[(row, col)]\n",
        "                action_probs = None\n",
        "                max_qsa = float(\"-inf\")\n",
        "                for action in range(4):\n",
        "                    next_state, reward, _, _ = env.simulate_step((row, col), action)\n",
        "                    qsa = reward + gamma * state_values[next_state]\n",
        "\n",
        "                    if qsa > max_qsa:\n",
        "                      max_qsa = qsa\n",
        "                      action_probs = np.zeros(4)\n",
        "                      action_probs[action] = 1.\n",
        "\n",
        "                state_values[(row, col)] = max_qsa\n",
        "                policy_probs[(row, col)] = action_probs\n",
        "\n",
        "                delta = max(delta, abs(max_qsa - old_value))"
      ],
      "metadata": {
        "id": "CFUksjcgDy11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Policy Iteration: a process that alternately improves the estimated values and the policy"
      ],
      "metadata": {
        "id": "nZI2S6MEH-0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_evaluation(policy_probs,state_values,theta=1e-6,gamma=0.99):\n",
        "    delta = float('inf')\n",
        "    while delta > theta:\n",
        "        delta = 0\n",
        "\n",
        "        for row in range(5):\n",
        "            for col in range(5):\n",
        "                old_value = state_values[(row,col)]\n",
        "                new_value = 0.\n",
        "                action_probabilities = policy_probs[(row,col)]\n",
        "\n",
        "                for action,prob in enumerate(action_probabilities):\n",
        "                    next_state,reward,_,_ = env.simulate_step((row,col),action)\n",
        "                    new_value += prob * (reward + gamma * state_values[next_state])\n",
        "\n",
        "                state_values[(row,col)] = new_value\n",
        "\n",
        "                delta = max(delta,abs(old_value-new_value))"
      ],
      "metadata": {
        "id": "JLvmwVSA-dHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_improvement(policy_probs,state_values,gamma=0.99):\n",
        "\n",
        "    policy_stable = True\n",
        "\n",
        "    for row in range(5):\n",
        "        for col in range(5):\n",
        "            old_action = policy_probs[(row,col)].argmax()\n",
        "\n",
        "            new_action = None\n",
        "            max_qsa = float('-inf')\n",
        "\n",
        "            for action in range(4):\n",
        "                next_state,reward,_,_ = env.simulate_step((row,col),action)\n",
        "                qsa = reward + gamma * state_values[next_state]\n",
        "\n",
        "                if qsa > max_qsa:\n",
        "                    new_action = action\n",
        "                    max_qsa = qsa\n",
        "\n",
        "            action_probs = np.zeros(4)\n",
        "            action_probs[new_action] = 1.\n",
        "            policy_probs[(row,col)] = action_probs\n",
        "\n",
        "            if new_action != old_action:\n",
        "                policy_stable = False\n",
        "\n",
        "    return policy_stable\n",
        ""
      ],
      "metadata": {
        "id": "QwcUqKPb-d8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_iteration(policy_probs,state_values,theta=1e-6,gamma=0.99):\n",
        "    policy_stable = False\n",
        "\n",
        "    while not policy_stable:\n",
        "\n",
        "        policy_evaluation(policy_probs,state_values,theta,gamma)\n",
        "        plot_values(state_values,frame)\n",
        "\n",
        "        policy_stable = policy_improvement(policy_probs,state_values,gamma)\n",
        "        plot_policy(policy_probs,frame)"
      ],
      "metadata": {
        "id": "EItBWi_T-i_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Essentially dynamic programming is finding the result of taking every action in every state in advance, without having to perform the action"
      ],
      "metadata": {
        "id": "2yv5jUH29JV1"
      }
    }
  ]
}